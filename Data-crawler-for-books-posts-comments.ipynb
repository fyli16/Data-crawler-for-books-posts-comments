{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab49f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import praw # Python Reddit API Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b596449",
   "metadata": {},
   "source": [
    "# Data crawling for books using requests & BeutifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f50f589a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped page 1\n",
      "Scraped page 2\n",
      "Scraped page 3\n",
      "Scraped page 4\n",
      "Scraped page 5\n",
      "{'title': 'A Light in the Attic', 'price': '£51.77', 'availability': 'In stock'}\n",
      "{'title': 'Tipping the Velvet', 'price': '£53.74', 'availability': 'In stock'}\n",
      "{'title': 'Soumission', 'price': '£50.10', 'availability': 'In stock'}\n",
      "{'title': 'Sharp Objects', 'price': '£47.82', 'availability': 'In stock'}\n",
      "{'title': 'Sapiens: A Brief History of Humankind', 'price': '£54.23', 'availability': 'In stock'}\n",
      "\n",
      "Total books scraped: 100\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = \"https://books.toscrape.com/\"\n",
    "START_URL = BASE_URL + \"catalogue/page-1.html\"\n",
    "\n",
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve: {url}\")\n",
    "        return None\n",
    "    return BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "def extract_book_info(book):\n",
    "    title = book.h3.a['title']\n",
    "    price = book.find('p', class_='price_color').text.strip()\n",
    "    availability = book.find('p', class_='instock availability').text.strip()\n",
    "    return {\n",
    "        'title': title,\n",
    "        'price': price,\n",
    "        'availability': availability\n",
    "    }\n",
    "\n",
    "def scrape_books(first_pages=5):\n",
    "    books_data = []\n",
    "    page_num = 1\n",
    "    while page_num <= first_pages:\n",
    "        page_url = f\"{BASE_URL}catalogue/page-{page_num}.html\"\n",
    "        soup = get_soup(page_url)\n",
    "        if not soup:\n",
    "            break\n",
    "        \n",
    "        book_items = soup.find_all('article', class_='product_pod')\n",
    "        if not book_items:\n",
    "            break  # No more books\n",
    "        \n",
    "        for book in book_items:\n",
    "            info = extract_book_info(book)\n",
    "            books_data.append(info)\n",
    "        \n",
    "        print(f\"Scraped page {page_num}\")\n",
    "        page_num += 1\n",
    "        time.sleep(1)  # Be polite\n",
    "\n",
    "    return books_data\n",
    "\n",
    "# Run and print the first 5 results\n",
    "all_books = scrape_books(first_pages=5)\n",
    "for book in all_books[:5]:\n",
    "    print(book)\n",
    "print(f\"\\nTotal books scraped: {len(all_books)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0431a5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>availability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Lumberjanes Vol. 3: A Terrible Plan (Lumberjan...</td>\n",
       "      <td>£19.92</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Layered: Baking, Building, and Styling Spectac...</td>\n",
       "      <td>£40.11</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Judo: Seven Steps to Black Belt (an Introducto...</td>\n",
       "      <td>£53.90</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Join</td>\n",
       "      <td>£35.67</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>In the Country We Love: My Family Divided</td>\n",
       "      <td>£22.00</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title   price availability\n",
       "0                                A Light in the Attic  £51.77     In stock\n",
       "1                                  Tipping the Velvet  £53.74     In stock\n",
       "2                                          Soumission  £50.10     In stock\n",
       "3                                       Sharp Objects  £47.82     In stock\n",
       "4               Sapiens: A Brief History of Humankind  £54.23     In stock\n",
       "..                                                ...     ...          ...\n",
       "95  Lumberjanes Vol. 3: A Terrible Plan (Lumberjan...  £19.92     In stock\n",
       "96  Layered: Baking, Building, and Styling Spectac...  £40.11     In stock\n",
       "97  Judo: Seven Steps to Black Belt (an Introducto...  £53.90     In stock\n",
       "98                                               Join  £35.67     In stock\n",
       "99          In the Country We Love: My Family Divided  £22.00     In stock\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(all_books)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d0cee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('scraped_books_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a85499c",
   "metadata": {},
   "source": [
    "# Data crawling for Reddit posts using API & praw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681cfe22",
   "metadata": {},
   "source": [
    "First, create reddit app to access the API: https://www.reddit.com/prefs/apps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b79a6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Authenticate\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"MZtrDHjJRgeivEtcMKmZhA\",\n",
    "    client_secret=\"vDFDHibQr6TnTVxUBevkXyRQyyNBRQ\",\n",
    "    user_agent=\"SpamDetectionBot/0.1 by hflsjl8swq\"\n",
    ")\n",
    "\n",
    "# Test read-only mode\n",
    "print(reddit.read_only)  # Should print: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e9c70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['STR_FIELD', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_additional_fetch_params', '_chunk', '_comments_by_id', '_edit_experimental', '_fetch', '_fetch_data', '_fetch_info', '_fetched', '_kind', '_reddit', '_replace_richtext_links', '_reset_attributes', '_safely_add_arguments', '_url_parts', '_vote', 'add_fetch_param', 'all_awardings', 'allow_live_comments', 'approved_at_utc', 'approved_by', 'archived', 'author', 'author_flair_background_color', 'author_flair_css_class', 'author_flair_richtext', 'author_flair_template_id', 'author_flair_text', 'author_flair_text_color', 'author_flair_type', 'author_fullname', 'author_is_blocked', 'author_patreon_flair', 'author_premium', 'award', 'awarders', 'banned_at_utc', 'banned_by', 'can_gild', 'can_mod_post', 'category', 'clear_vote', 'clicked', 'comment_limit', 'comment_sort', 'comments', 'content_categories', 'contest_mode', 'created', 'created_utc', 'crosspost', 'delete', 'disable_inbox_replies', 'discussion_type', 'distinguished', 'domain', 'downs', 'downvote', 'duplicates', 'edit', 'edited', 'enable_inbox_replies', 'flair', 'fullname', 'gild', 'gilded', 'gildings', 'hidden', 'hide', 'hide_score', 'id', 'id_from_url', 'is_created_from_ads_ui', 'is_crosspostable', 'is_meta', 'is_original_content', 'is_reddit_media_domain', 'is_robot_indexable', 'is_self', 'is_video', 'likes', 'link_flair_background_color', 'link_flair_css_class', 'link_flair_richtext', 'link_flair_template_id', 'link_flair_text', 'link_flair_text_color', 'link_flair_type', 'locked', 'mark_visited', 'media', 'media_embed', 'media_only', 'mod', 'mod_note', 'mod_reason_by', 'mod_reason_title', 'mod_reports', 'name', 'no_follow', 'num_comments', 'num_crossposts', 'num_reports', 'over_18', 'parse', 'permalink', 'pinned', 'post_hint', 'preview', 'pwls', 'quarantine', 'removal_reason', 'removed_by', 'removed_by_category', 'reply', 'report', 'report_reasons', 'save', 'saved', 'score', 'secure_media', 'secure_media_embed', 'selftext', 'selftext_html', 'send_replies', 'shortlink', 'spoiler', 'stickied', 'subreddit', 'subreddit_id', 'subreddit_name_prefixed', 'subreddit_subscribers', 'subreddit_type', 'suggested_sort', 'thumbnail', 'thumbnail_height', 'thumbnail_width', 'title', 'top_awarded_type', 'total_awards_received', 'treatment_tags', 'unhide', 'unsave', 'ups', 'upvote', 'upvote_ratio', 'url', 'url_overridden_by_dest', 'user_reports', 'view_count', 'visited', 'wls']\n"
     ]
    }
   ],
   "source": [
    "subreddit = reddit.subreddit(\"technology\")  # or a specific one like \"technology\"\n",
    "# check attributes of the subreddit post\n",
    "for post in subreddit.new(limit=1):\n",
    "    print(dir(post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "194a305c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Autonomous AI systems can help tackle global food insecurity', 'author': 'upyoars', 'score': 0, 'num_comments': 0, 'url': 'https://phys.org/news/2025-06-autonomous-ai-tackle-global-food.html', 'is_original_content': False}\n",
      "{'title': 'Deadline for Getting Payment on Azure Power $23M Settlement Is in a Few Weeks', 'author': '11thestate', 'score': 2, 'num_comments': 0, 'url': 'https://11th.com/cases/azure-investor-settlement', 'is_original_content': False}\n",
      "{'title': '‘Fortnite’ Players to Receive More Than $126 Million in Refunds From FTC', 'author': 'a_Ninja_b0y', 'score': 10, 'num_comments': 3, 'url': 'https://variety.com/2025/gaming/news/how-to-get-fortnite-ftc-refund-1236441827/', 'is_original_content': False}\n",
      "{'title': 'How Cops Can Get Your Private Online Data', 'author': 'SaveDnet-FRed0', 'score': 1, 'num_comments': 4, 'url': 'https://www.eff.org/deeplinks/2025/06/how-cops-can-get-your-private-online-data', 'is_original_content': False}\n",
      "{'title': 'Xiaomi YU7 over 200,000 firm orders in the first 3 minutes', 'author': 'Sartew', 'score': 8, 'num_comments': 1, 'url': 'https://cnevpost.com/2025/06/26/xiaomi-launches-yu7-suv/', 'is_original_content': False}\n"
     ]
    }
   ],
   "source": [
    "# Get the 500 newest posts from a subreddit\n",
    "subreddit = reddit.subreddit(\"technology\")  # or a specific one like \"technology\"\n",
    "for post in subreddit.new(limit=5):\n",
    "    print({\n",
    "        \"title\": post.title,\n",
    "        # \"text\": post.selftext,\n",
    "        \"author\": str(post.author),\n",
    "        \"score\": post.score,\n",
    "        \"num_comments\": post.num_comments,\n",
    "        \"url\": post.url,\n",
    "        \"is_original_content\": post.is_original_content,\n",
    "    })\n",
    "\n",
    "# save to CSV\n",
    "with open(\"reddit_new_posts.csv\", \"w\", newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"title\", \"author\", \"score\", \"num_comments\", \"url\", \"is_original_content\"])\n",
    "\n",
    "    for post in subreddit.new(limit=500):\n",
    "        writer.writerow([post.title, str(post.author), post.score, post.num_comments, post.url, post.is_original_content])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d46abfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'comment_body': 'still a BSOD, black screen of death........ok', 'author': 'sixbone', 'score': 1, 'permalink': '/r/technology/comments/1ll4fin/windows_is_getting_rid_of_the_blue_screen_of/mzxf9ck/'}\n",
      "{'comment_body': \"That's simultaneously the most and least believable thing in the world\", 'author': 'Accurate_Koala_4698', 'score': 1, 'permalink': '/r/technology/comments/1ll6a2j/salesforce_ceo_claims_half_of_the_companys_work/mzxf90r/'}\n",
      "{'comment_body': \"if he will become a hobo due to it at some point, the society is going to suffer. also right now he isn't as productive as he could be. the society is already suffering. it's selfish\", 'author': 'polacy_do_pracy', 'score': 1, 'permalink': '/r/technology/comments/1lka83a/bernie_sanders_says_that_if_ai_makes_us_so/mzxf8z4/'}\n",
      "{'comment_body': 'This country is run by morons, and sleezebags.', 'author': 'FoggyGanj', 'score': 1, 'permalink': '/r/technology/comments/1ll51e9/critical_hurricane_forecast_tool_abruptly/mzxf8qb/'}\n",
      "{'comment_body': \"you would drive through a flashing red school bus stop sign w/o stopping? \\n\\nand then after hitting a kid so its trapped under your car.... keep driving?\\n\\nand other self driving cars that use sensors (not just cameras) like lidar... don't fall for the fake wall. it was making a point about how dumb/bad the self driving is... what if there was a mural, or a bus ad, etc\", 'author': 'hobbykitjr', 'score': 1, 'permalink': '/r/technology/comments/1ll1553/france_orders_tesla_to_halt_misleading_marketing/mzxf8dc/'}\n"
     ]
    }
   ],
   "source": [
    "# Get the 5 newest comments from a subreddit\n",
    "for comment in subreddit.comments(limit=5):\n",
    "    print({\n",
    "        \"comment_body\": comment.body,\n",
    "        \"author\": str(comment.author),\n",
    "        \"score\": comment.score,\n",
    "        \"permalink\": comment.permalink\n",
    "    })\n",
    "\n",
    "# Save comments to CSV\n",
    "with open(\"reddit_new_comments.csv\", \"w\", newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"comment_body\", \"author\", \"score\", \"permalink\"])\n",
    "\n",
    "    for comment in subreddit.new(limit=500):\n",
    "        writer.writerow([comment.title, str(comment.author), comment.score, comment.permalink])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
